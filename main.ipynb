{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 디렉토리 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import os \n",
    "import shutil\n",
    "x=dt.datetime.now()\n",
    "day=str(x.year)+'_0'+str(x.month)+'_'+str(x.day)\n",
    "\n",
    "if not os.path.exists(day):\n",
    "    os.makedirs(day)\n",
    "    shutil.move('./remains.csv', './'+day+'/remains.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIKI ID 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mkwikidata\n",
    "import json\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_ticker={'BALL':'Q4034815','BF-B':'Q392221','BRK-B':'Q217583','KDP':'Q3116111','WTW':'Q21189883'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp480=pd.read_csv('./'+day+'/remains.csv')\n",
    "ticker_symbol=dict()\n",
    "count=0\n",
    "for ticker in sp480:\n",
    "    query='''\n",
    "    SELECT ?company\n",
    "\n",
    "    WHERE {\n",
    "    \n",
    "    ?company p:P414[pq:P249 '%s'] .\n",
    "    \n",
    "    SERVICE wikibase:label{\n",
    "        bd:serviceParam wikibase:language \"en\"  .\n",
    "        }\n",
    "    }\n",
    "    '''%ticker\n",
    "    try:\n",
    "        query_result=mkwikidata.run_query(query,params={})\n",
    "        ticker_symbol[ticker]=query_result['results']['bindings'][0]['company']['value'][31:]\n",
    "    except:\n",
    "        if ticker not in none_ticker.keys():\n",
    "            print(ticker)\n",
    "        else:\n",
    "            ticker_symbol[ticker]=none_ticker[ticker]\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./'+day+'/wiki_ID.json','w') as f:\n",
    "    json.dump(ticker_symbol,f,indent=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "with open(\"./\"+day+\"/wiki_ID.json\") as f:\n",
    "    ticker_entityID=json.load(f) \n",
    "data=[]\n",
    "for key in ticker_entityID.keys():\n",
    "    sample={}\n",
    "    sample['ticker']=key\n",
    "    sample['wiki_code']=ticker_entityID[key]\n",
    "    data.append(sample)\n",
    "df=pd.DataFrame(data,columns=['ticker','wiki_code'])\n",
    "df.to_csv('./'+day+'/company_480.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### triple 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awena\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_list={'P127':'P1830','P1830':'P127','P155':'P156','P156':'P155',\n",
    "'P355':'P749','P749':'P355'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./\"+day+\"/wiki_ID.json\") as f:\n",
    "    ticker_entityID=json.load(f)\n",
    "\n",
    "crawler\t\t= awena.Crawler('en') # set language of labels and descriptions\n",
    "data=[]\n",
    "count=0\n",
    "for ticker,wiki_code in ticker_entityID.items():\n",
    "    \n",
    "    info= crawler.load(wiki_code)\n",
    "    for relation,node in info.items():\n",
    "        # print(relation)\n",
    "        if node=='None' or relation=='label' or relation=='id' or relation=='ticker':\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            for i in node:\n",
    "                sample=list()\n",
    "                sample.extend([wiki_code,relation,i])\n",
    "     \n",
    "                if sample not in data:\n",
    "                    data.append(sample)\n",
    "            if relation in reverse_list.keys():\n",
    "                for i in node:\n",
    "                    sample=list()\n",
    "                    sample.extend([i,reverse_list[relation],wiki_code])\n",
    "                    if sample not in data:\n",
    "                        data.append(sample)\n",
    "\n",
    "data_dict=[]\n",
    "for sample in data:\n",
    "    triple={}\n",
    "    triple['wiki_code']=sample[0]\n",
    "    triple['relation']=sample[1]\n",
    "    triple['node']=sample[2]\n",
    "    data_dict.append(triple)\n",
    "df=pd.DataFrame(data_dict,columns=['wiki_code','relation','node'])\n",
    "df.to_csv('./'+day+'/triple.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# path = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "path='https://en.wikipedia.org/w/index.php?title=List_of_S%26P_500_companies&oldid=1120818591'\n",
    "companies = pd.read_html(path, flavor='bs4')[0].set_index('Symbol')[['GICS Sector']]\n",
    "companies['temp']=companies.index\n",
    "companies.loc[['BF.B'],['temp']]='BF-B'\n",
    "companies.loc[['BRK.B'],['temp']]='BRK-B'\n",
    "companies=companies.set_index('temp',drop=True)\n",
    "companies=companies.rename_axis('Symbol')\n",
    "with open('./'+day+'/wiki_ID.json', 'r') as fo:\n",
    "    ticker = json.load(fo)\n",
    "\n",
    "ticker=list(ticker.keys())\n",
    "ticker.sort()\n",
    "df=companies.loc[ticker]\n",
    "df.rename(columns={\"GICS Sector\":\"GICS\"},inplace=True)\n",
    "df.to_csv('./'+day+'/GICS.csv')\n",
    "df.reset_index(inplace=True)\n",
    "df.drop('Symbol',axis=1,inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.to_csv('./'+day+'/GICS_type.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# path = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "path='https://en.wikipedia.org/w/index.php?title=List_of_S%26P_500_companies&oldid=1120818591'\n",
    "companies = pd.read_html(path, flavor='bs4')[0].set_index('Symbol')[['GICS Sub-Industry']]\n",
    "companies['temp']=companies.index\n",
    "companies.loc[['BF.B'],['temp']]='BF-B'\n",
    "companies.loc[['BRK.B'],['temp']]='BRK-B'\n",
    "companies=companies.set_index('temp',drop=True)\n",
    "companies=companies.rename_axis('Symbol')\n",
    "# companies.drop(companies[companies['GICS Sub-Industry']=='Other Services'].index,inplace=True)\n",
    "with open('./'+day+'/wiki_ID.json', 'r') as fo:\n",
    "    ticker = json.load(fo)\n",
    "\n",
    "ticker=list(ticker.keys())\n",
    "ticker.sort()\n",
    "df=companies.loc[ticker]\n",
    "\n",
    "df_split1=df['GICS Sub-Industry'].str.split(' & ',expand=True)\n",
    "df_split2=df_split1[0].str.split(', ',expand=True)\n",
    "new=pd.concat([df_split1[1],df_split1[2],df_split2[0],df_split2[1]])\n",
    "new=new.dropna().to_frame()\n",
    "new.rename(columns={0:\"GICS_SUB\"},inplace=True)\n",
    "new.drop(new[new['GICS_SUB']=='Other Services'].index,inplace=True)\n",
    "new.to_csv('./'+day+'/GICS_SUB.csv')\n",
    "new.reset_index(inplace=True)\n",
    "new.drop('Symbol',axis=1,inplace=True)\n",
    "new.drop_duplicates(inplace=True)\n",
    "new.to_csv('./'+day+'/GICS_SUB_type.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_driver import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''LOAD CSV WITH HEADERS FROM 'file:///%s/company_480.csv' as  line\n",
    "create (:company{ticker:line.ticker,wiki_code:line.wiki_code,period:'%s'})\n",
    "'''%(day,day)\n",
    "run_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''LOAD CSV WITH HEADERS FROM 'file:///%s/triple.csv' as line\n",
    "merge (node:company{wiki_code:line.node,period:'%s'})\n",
    "on create set node:not_company,node.wiki_code=line.node,node.period='%s'\n",
    "'''%(day,day,day)\n",
    "run_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''match (n:not_company{period:'%s'})\n",
    "remove n:company\n",
    "'''%(day)\n",
    "run_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''LOAD CSV WITH HEADERS FROM 'file:///%s/triple.csv' as  line\n",
    "match( node1:company{wiki_code:line.wiki_code,period:'%s'})\n",
    "match( node2 {wiki_code:line.node,period:'%s'})\n",
    "create (node1)-[:relation{relation:line.relation}]->(node2)\n",
    "'''%(day,day,day)\n",
    "run_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''LOAD CSV WITH HEADERS FROM 'file:///%s/GICS.csv' as line\n",
    "merge (node:GICS{field:line.GICS,period:'%s'})\n",
    "on create set node:GICS,node.field=line.GICS,node.period='%s'\n",
    "'''%(day,day,day)\n",
    "run_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''LOAD CSV WITH HEADERS FROM 'file:///%s/GICS_SUB.csv' as line\n",
    "merge (node:GICS_SUB{field:line.GICS_SUB,period:'%s'})\n",
    "on create set node:GICS_SUB,node.field=line.GICS_SUB,node.period='%s'\n",
    "'''%(day,day,day)\n",
    "run_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''LOAD CSV WITH HEADERS FROM 'file:///%s/GICS.csv' as line\n",
    "match (node1:company{ticker:line.Symbol,period:'%s'})\n",
    "match (node2:GICS{field:line.GICS,period:'%s'})\n",
    "create (node2)-[:GICS_R]->(node1)\n",
    "'''%(day,day,day)\n",
    "run_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''LOAD CSV WITH HEADERS FROM 'file:///%s/GICS_SUB.csv' as line\n",
    "match (node1:company{ticker:line.Symbol,period:'%s'})\n",
    "match (node2:GICS_SUB{field:line.GICS_SUB,period:'%s'})\n",
    "create (node2)-[:GICS_SUB_R]->(node1)\n",
    "'''%(day,day,day)\n",
    "run_query(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Graph 생성 - first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_driver import *\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''LOAD CSV WITH HEADERS FROM 'file:///%s/company_480.csv' as line\n",
    "match(node:company{ticker:line.ticker,period:'%s'})\n",
    "with collect(node) as nodes\n",
    "LOAD CSV WITH HEADERS FROM 'file:///data/first_order.csv' as line\n",
    "with nodes,line\n",
    "unwind nodes as node1\n",
    "unwind nodes as node2\n",
    "WITH line.r as edge_r,node1,node2,\n",
    "CASE\n",
    "    WHEN exists((node1)-[:relation{relation:line.r}]->(node2)) then 1\n",
    "    ELSE 0\n",
    "END AS overlap\n",
    "return edge_r,collect(overlap)\n",
    "\n",
    "'''%(day,day)\n",
    "result=run_query_extraction(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies=pd.read_csv('./'+day+'/company_480.csv')\n",
    "length=len(companies)\n",
    "H=None\n",
    "for line in result:\n",
    "    if line['collect(overlap)'].count(1)<2:\n",
    "        continue\n",
    "    matrix=np.array(line['collect(overlap)']).reshape(length,length)\n",
    "    if H is None:\n",
    "        H=matrix[None,:]\n",
    "    else:\n",
    "        H=np.vstack((H,matrix[None,:]))\n",
    "print(H.shape)\n",
    "np.save('./'+day+'/first_simple.npy',H)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Graph 생성 - Second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_driver import *\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''LOAD CSV WITH HEADERS FROM 'file:///%s/company_480.csv' as line\n",
    "match(node:company{ticker:line.ticker,period:'%s'})\n",
    "with collect(node) as nodes\n",
    "LOAD CSV WITH HEADERS FROM 'file:///data/second_not_arrange.csv' as line\n",
    "with nodes,line\n",
    "unwind nodes as node1\n",
    "unwind nodes as node2\n",
    "WITH line.r1 as edge_r1,line.r2 as edge_r2,node1,node2,\n",
    "CASE\n",
    "    WHEN exists((node1)-[:relation{relation:line.r1}]->()<-[:relation{relation:line.r2}]-(node2)) and node1.wiki_code <> node2.wiki_code then 1\n",
    "    ELSE 0\n",
    "END AS overlap\n",
    "return edge_r1,edge_r2,collect(overlap)\n",
    "\n",
    "'''%(day,day)\n",
    "result=run_query_extraction(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies=pd.read_csv('./'+day+'/company_480.csv')\n",
    "length=len(companies)\n",
    "H=None\n",
    "for line in result:\n",
    "    if line['collect(overlap)'].count(1)<2:\n",
    "        continue\n",
    "    matrix=np.array(line['collect(overlap)']).reshape(length,length)\n",
    "    if H is None:\n",
    "        H=matrix[None,:]\n",
    "    else:\n",
    "        H=np.vstack((H,matrix[None,:]))\n",
    "print(H.shape)\n",
    "np.save('./'+day+'/second_simple.npy',H)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Graph - Sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_driver import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=''' LOAD CSV WITH HEADERS FROM 'file:///%s/company_480.csv' as line\n",
    "match(node:company{ticker:line.ticker,period:'%s'})\n",
    "with collect(node) as nodes\n",
    "LOAD CSV WITH HEADERS FROM 'file:///%s/GICS_type.csv' as line\n",
    "with nodes,line\n",
    "unwind nodes as node1\n",
    "unwind nodes as node2\n",
    "with line.GICS as gics,node1,node2,\n",
    "case\n",
    "    when node1.ticker<>node2.ticker and exists((:GICS{field:line.GICS,period:'%s'})-[:GICS_R]->(node1)) and exists(({field:line.GICS,period:'%s'})-[:GICS_R]->(node2)) then 1\n",
    "    else 0\n",
    "end as overlap\n",
    "return gics,collect(overlap)\n",
    "\n",
    "'''%(day,day,day,day,day)\n",
    "result=run_query_extraction(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies=pd.read_csv('./'+day+'/company_480.csv')\n",
    "length=len(companies)\n",
    "H=None\n",
    "for line in result:\n",
    "    matrix=np.array(line['collect(overlap)']).reshape(length,length)\n",
    "    if H is None:\n",
    "        H=matrix[None,:]\n",
    "    else:\n",
    "        H=np.vstack((H,matrix[None,:]))\n",
    "print(H.shape)\n",
    "np.save('./'+day+'/sector_simple.npy',H)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Graph - first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_driver import *\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=''' LOAD CSV WITH HEADERS FROM 'file:///data/first_order.csv' as line\n",
    "match (node1:company{period:'%s'})-[r:relation{relation:line.r}]->(node2:company{period:'%s'})\n",
    "return node1.wiki_code,r.relation,node2.wiki_code\n",
    "'''%(day,day)\n",
    "result=run_query_extraction(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=list()\n",
    "for line in result:\n",
    "    sample=dict()\n",
    "    sample['object']=line['node1.wiki_code']\n",
    "    sample['r']=line['r.relation']\n",
    "    data.append(sample)\n",
    "df=pd.DataFrame(data,columns=['object','r'])\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.to_csv('./'+day+'/first_hyperedge.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=''' LOAD CSV WITH HEADERS FROM 'file:///%s/company_480.csv' as line\n",
    "match(n:company{ticker:line.ticker,period:'%s'})\n",
    "with collect(n) as nodes\n",
    "LOAD CSV WITH HEADERS FROM 'file:///%s/first_hyperedge.csv' as line\n",
    "with nodes,line\n",
    "unwind nodes as node\n",
    "optional match path=(:company{wiki_code:line.object,period:'%s'})-[:relation{relation:line.r}]->(node)\n",
    "with line.object as edge_n,line.r as edge_r,path,\n",
    "CASE\n",
    "    WHEN path is null then 0\n",
    "    ELSE 1\n",
    "END AS overlap \n",
    "return edge_n,edge_r,collect(overlap)\n",
    "'''%(day,day,day,day)\n",
    "result=run_query_extraction(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies=pd.read_csv('./'+day+'/company_480.csv')\n",
    "data=dict()\n",
    "columns=list()\n",
    "count_list=list()\n",
    "count=0\n",
    "for line in result:\n",
    "    if line['collect(overlap)'].count(1)<2:\n",
    "        count+=1\n",
    "        continue\n",
    "    else:\n",
    "        count_list.append(line['collect(overlap)'].count(1))\n",
    "    column=str(line['edge_n'])+':'+str(line['edge_r'])\n",
    "    data[column]=line['collect(overlap)']\n",
    "    columns.append(column)\n",
    "df=pd.DataFrame(data,columns=columns,index=companies['ticker'])\n",
    "df.to_csv('./'+day+'/first_hyper.csv')\n",
    "print(count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Graph - Second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_driver import *\n",
    "import csv\n",
    "import pandas  as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=''' LOAD CSV WITH HEADERS FROM 'file:///data/second_order_arrange.csv' as line\n",
    "match (node1:company{period:'%s'})-[r1:relation{relation:line.r1}]->(bridge{period:'%s'})<-[r2:relation{relation:line.r2}]-(node2:company{period:'%s'})\n",
    "return r1.relation,r2.relation,bridge.wiki_code,node1.wiki_code,node2.wiki_code\n",
    "order by r1.relation,r2.relation,bridge.wiki_code\n",
    "'''%(day,day,day)\n",
    "result=run_query_extraction(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=list()\n",
    "for line in result:\n",
    "    sample=dict()\n",
    "    sample['r1']=line['r1.relation']\n",
    "    sample['bridge']=line['bridge.wiki_code']\n",
    "    sample['r2']=line['r2.relation']\n",
    "    data.append(sample)\n",
    "df=pd.DataFrame(data,columns=['r1','bridge','r2'])\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.to_csv('./'+day+'/second_hyperedge.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day='2023_01_15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=''' LOAD CSV WITH HEADERS FROM 'file:///%s/company_480.csv' as line\n",
    "match(n:company{ticker:line.ticker,period:'%s'})\n",
    "with collect(n) as nodes\n",
    "LOAD CSV WITH HEADERS FROM 'file:///%s/second_hyperedge.csv' as line\n",
    "with nodes,line\n",
    "unwind nodes as node1 \n",
    "with line.r1 as edge_r1,line.r2 as edge_r2,line.bridge as edge_n,node1,\n",
    "CASE\n",
    "    WHEN exists((node1)-[:relation{relation:line.r1}]->({wiki_code:line.bridge})<-[:relation{relation:line.r2}]-(:company)) then 1\n",
    "    WHEN exists((:company)-[:relation{relation:line.r1}]->({wiki_code:line.bridge})<-[:relation{relation:line.r2}]-(node1)) then 1\n",
    "    ELSE 0\n",
    "END AS overlap \n",
    "return edge_r1,edge_r2,edge_n,collect(overlap)\n",
    "'''%(day,day,day)\n",
    "result=run_query_extraction(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies=pd.read_csv('./'+day+'/company_480.csv')\n",
    "data=dict()\n",
    "columns=list()\n",
    "count_list=list()\n",
    "count=0\n",
    "for line in result:\n",
    "    if line['collect(overlap)'].count(1)<2:\n",
    "        count+=1\n",
    "        continue\n",
    "    else:\n",
    "        count_list.append(line['collect(overlap)'].count(1))\n",
    "    column=str(line['edge_r1'])+':'+str(line['edge_n'])+':'+str(line['edge_r2'])\n",
    "    data[column]=line['collect(overlap)']\n",
    "    columns.append(column)\n",
    "df=pd.DataFrame(data,columns=columns,index=companies['ticker'])\n",
    "df.to_csv('./'+day+'/second_hyper.csv')\n",
    "print(count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Graph - Sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_driver import *\n",
    "import csv\n",
    "import pandas  as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=''' LOAD CSV WITH HEADERS FROM 'file:///%s/company_480.csv' as line\n",
    "match(node:company{ticker:line.ticker,period:'%s'})\n",
    "with collect(node) as nodes\n",
    "LOAD CSV WITH HEADERS FROM 'file:///%s/GICS_type.csv' as line\n",
    "unwind nodes as node\n",
    "with line.GICS as gics,node,\n",
    "case\n",
    "    when exists((:GICS{field:line.GICS,period:'%s'})-[:GICS_R]->(node)) then 1\n",
    "    else 0\n",
    "end as overlap\n",
    "return gics,collect(overlap)\n",
    "'''%(day,day,day,day)\n",
    "result=run_query_extraction(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies=pd.read_csv('./'+day+'/company_480.csv')\n",
    "data=dict()\n",
    "columns=list()\n",
    "for line in result:\n",
    "    if line['collect(overlap)'].count(1)<2:\n",
    "        continue\n",
    "    data[line['gics']]=line['collect(overlap)']\n",
    "    columns.append(line['gics'])\n",
    "GICS=pd.DataFrame(data,columns=columns,index=companies['ticker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=''' LOAD CSV WITH HEADERS FROM 'file:///%s/company_480.csv' as line\n",
    "match(node:company{ticker:line.ticker,period:'%s'})\n",
    "with collect(node) as nodes\n",
    "LOAD CSV WITH HEADERS FROM 'file:///%s/GICS_SUB_type.csv' as line\n",
    "unwind nodes as node\n",
    "with line.GICS_SUB as gics_sub,node,\n",
    "case\n",
    "    when exists((:GICS_SUB{field:line.GICS_SUB,period:'%s'})-[:GICS_SUB_R]->(node)) then 1\n",
    "    else 0\n",
    "end as overlap\n",
    "return gics_sub,collect(overlap)\n",
    "'''%(day,day,day,day)\n",
    "result=run_query_extraction(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies=pd.read_csv('./'+day+'/company_480.csv')\n",
    "data=dict()\n",
    "columns=list()\n",
    "for line in result:\n",
    "    if line['collect(overlap)'].count(1)<2:\n",
    "        continue\n",
    "    data[line['gics_sub']]=line['collect(overlap)']\n",
    "    columns.append(line['gics_sub'])\n",
    "GICS_SUB=pd.DataFrame(data,columns=columns,index=companies['ticker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECTOR=pd.concat([GICS,GICS_SUB],axis=1,join='inner')\n",
    "SECTOR.to_csv('./'+day+'/sector_hyper.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 중복제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_graph(a):\n",
    "    tmp = a.reshape(a.shape[0], -1)\n",
    "    b = np.ascontiguousarray(tmp).view(np.dtype((np.void, tmp.dtype.itemsize * tmp.shape[1])))\n",
    "    _, idx = np.unique(b, return_index=True)\n",
    "    return a[idx].reshape(-1, *a.shape[1:])\n",
    "\n",
    "\n",
    "def load_Graph_data(day):\n",
    "    G,H=None,None\n",
    "    try:\n",
    "        g1=np.load('./'+day+'/sector_simple.npy')\n",
    "        g2=np.load('./'+day+'/first_simple.npy')\n",
    "        g3=np.load('./'+day+'/second_simple.npy')\n",
    "        g123=np.concatenate((g1,g2,g3),0)\n",
    "        G=unique_graph(g123)\n",
    "        print('Simple graph shape: ',G.shape)\n",
    "    except Exception:\n",
    "        print('No simple graph')\n",
    "\n",
    "    try:\n",
    "        h1=pd.read_csv('./'+day+'/sector_hyper.csv',index_col='ticker')\n",
    "        h2=pd.read_csv('./'+day+'/first_hyper.csv',index_col='ticker')\n",
    "        h3=pd.read_csv('./'+day+'/second_hyper.csv',index_col='ticker')\n",
    "        h123=h1.join(h2,how='left').join(h3,how='left')\n",
    "        H=h123.loc[:,~h123.T.duplicated()].dropna(axis=1).values\n",
    "        print('Hypergraph shape: ',H.shape)\n",
    "    except Exception:\n",
    "        print('No hypergraph')\n",
    "    \n",
    "    return G,H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_Graph_data(day)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce2c101e633a132a2be35be8e919a9e0dab56f707b035eb5d9ffe17010837cfa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
