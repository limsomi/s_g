{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 디렉토리 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil\n",
    "num=1\n",
    "if not os.path.exists('./history'):\n",
    "    os.makedirs('./history')\n",
    "while(True):\n",
    "    PHASE='phase'+str(num)\n",
    "    day='./history/'+PHASE\n",
    "    if not os.path.exists(day):\n",
    "        os.makedirs(day)\n",
    "        shutil.move('./'+PHASE+'.csv',day+'/'+PHASE+'.csv')\n",
    "        break\n",
    "    else:\n",
    "        num+=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIKI ID 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mkwikidata\n",
    "import json\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase1=pd.read_csv('./history/phase1/phase1.csv')\n",
    "phase2=pd.read_csv('./history/phase2/phase2.csv')\n",
    "phase3=pd.read_csv('./history/phase3/phase3.csv')\n",
    "phase4=pd.read_csv('./history/phase4/phase4.csv')\n",
    "phase5=pd.read_csv('./history/phase5/phase5.csv')\n",
    "phase6=pd.read_csv('./history/phase6/phase6.csv')\n",
    "phase7=pd.read_csv('./history/phase7/phase7.csv')\n",
    "phase8=pd.read_csv('./history/phase8/phase8.csv')\n",
    "phase9=pd.read_csv('./history/phase9/phase9.csv')\n",
    "\n",
    "sp480=pd.merge(phase1,phase2)\n",
    "sp480=pd.merge(sp480,phase3)\n",
    "sp480=pd.merge(sp480,phase4)\n",
    "sp480=pd.merge(sp480,phase5)\n",
    "sp480=pd.merge(sp480,phase6)\n",
    "sp480=pd.merge(sp480,phase7)\n",
    "sp480=pd.merge(sp480,phase8)\n",
    "sp480=pd.merge(sp480,phase9)\n",
    "sp480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_ticker={'BALL':'Q4034815','BF-B':'Q392221','BRK-B':'Q217583','KDP':'Q3116111','WTW':'Q21189883'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_symbol=dict()\n",
    "count=0\n",
    "for ticker in sp480:\n",
    "    query='''\n",
    "    SELECT ?company\n",
    "\n",
    "    WHERE {\n",
    "    \n",
    "    ?company p:P414[pq:P249 '%s'] .\n",
    "    \n",
    "    SERVICE wikibase:label{\n",
    "        bd:serviceParam wikibase:language \"en\"  .\n",
    "        }\n",
    "    }\n",
    "    '''%ticker\n",
    "    try:\n",
    "        query_result=mkwikidata.run_query(query,params={})\n",
    "        ticker_symbol[ticker]=query_result['results']['bindings'][0]['company']['value'][31:]\n",
    "    except:\n",
    "        try:\n",
    "            query_result=mkwikidata.run_query(query,params={})\n",
    "            ticker_symbol[ticker]=query_result['results']['bindings'][0]['company']['value'][31:]\n",
    "        except:\n",
    "            if ticker not in none_ticker.keys():\n",
    "                print(query_result)\n",
    "                print(ticker)\n",
    "            else:\n",
    "                ticker_symbol[ticker]=none_ticker[ticker]\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./history/wiki_ID.json','w') as f:\n",
    "    json.dump(ticker_symbol,f,indent=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### timestamp와 id 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./history/wiki_ID.json') as f:\n",
    "    ticker=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "relation=['P6379', 'P749', 'P366', 'P739', 'P121', 'P4950', \n",
    "'P131', 'P495', 'P625', 'P355', 'P463', 'P5009', 'P17', 'P414',\n",
    " 'P1830', 'P452', 'P306', 'P176', 'P169', 'P199', 'P1344', 'P156',\n",
    "  'P2770', 'P3320', 'P113', 'P1454', 'P159', 'P740', 'P114', 'P31',\n",
    "   'P1056', 'P1889', 'P166', 'P400', 'P361', 'P155', 'P127', 'P112', 'P793']\n",
    "SYMBOL=dict()\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "for Symbol,ID in ticker.items():\n",
    "    \n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\":\"json\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"titles\": ID,\n",
    "        \"rvprop\": \"ids|timestamp|flags|comment|user\",\n",
    "        \"rvlimit\":40000\n",
    "    }\n",
    "\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    DATA = R.json()\n",
    "\n",
    "    PAGES = DATA[\"query\"][\"pages\"]\n",
    "    # PAGES\n",
    "    day_id=list()\n",
    "    for page in PAGES.keys():\n",
    "        for i in range(len(PAGES[page]['revisions'])):\n",
    "            for r in relation:\n",
    "                try: \n",
    "                    property=\"[Property:\"+str(r)+\"]\"\n",
    "                    if property in  PAGES[page]['revisions'][i]['comment']:\n",
    "                        sample=dict()\n",
    "                        sample['timestamp']=str(PAGES[page]['revisions'][i]['timestamp'])\n",
    "                        sample['revid']=PAGES[page]['revisions'][i]['revid']\n",
    "                        day_id.append(sample)\n",
    "                except:\n",
    "                    continue\n",
    "    SYMBOL[Symbol]=list(reversed(day_id))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "def add_data(year,month,revid,sample_list):\n",
    "    sample=dict()\n",
    "    d=dt.datetime(year,month,1,1)\n",
    "    sample[\"timestamp\"]=d.strftime('%Y-%m')\n",
    "    sample[\"revid\"]=revid\n",
    "    sample_list.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_json=dict()\n",
    "for TICKER,DATA in SYMBOL.items():\n",
    "    sample_list=list()\n",
    "    year=2014\n",
    "    month=1\n",
    "    revid=DATA[0]['revid']\n",
    "    for data in DATA:\n",
    "        json_year=int(data['timestamp'][:4])\n",
    "        json_month=int(data['timestamp'][5:7])\n",
    "        if year==2014 and json_year==2015:\n",
    "            while json_month!=month:\n",
    "                add_data(2015,month,revid,sample_list)\n",
    "                month+=1\n",
    "            year=json_year\n",
    "        elif year!=2014 and  json_year==year and json_month>month:\n",
    "            while json_month!=month:\n",
    "                add_data(year,month,revid,sample_list)\n",
    "                month+=1\n",
    "            year=json_year\n",
    "        elif year!=2014 and json_year>year:\n",
    "            while year!=json_year:\n",
    "                while month!=12:\n",
    "                    add_data(year,month,revid,sample_list)\n",
    "                    month+=1\n",
    "                add_data(year,month,revid,sample_list)\n",
    "                year+=1\n",
    "                month=1\n",
    "            while month!=json_month:\n",
    "                add_data(year,month,revid,sample_list)\n",
    "                month+=1\n",
    "        revid=data['revid']\n",
    "    while year!=dt.datetime.today().year:\n",
    "        while month!=12:\n",
    "            add_data(year,month,revid,sample_list)\n",
    "            month+=1\n",
    "        add_data(year,month,revid,sample_list)\n",
    "        year+=1\n",
    "        month=1\n",
    "    while month!=dt.datetime.today().month:\n",
    "        add_data(year,month,revid,sample_list)\n",
    "        month+=1\n",
    "    new_json[TICKER]=sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_json['ZTS']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 월별로 ID 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "TOTAL_DATA=dict()\n",
    "for i in range(96):\n",
    "    sample=dict()\n",
    "    for TICKER,DATA in new_json.items():\n",
    "        date=DATA[i]['timestamp']\n",
    "        sample[TICKER]=DATA[i]['revid']\n",
    "    TOTAL_DATA[date]=sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./history/timestamp.json','w',encoding='UTF-8') as file:\n",
    "    json.dump(TOTAL_DATA,file,indent=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### revid로 과거 트리플 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "with open ('./history/timestamp.json','r') as f:\n",
    "    DATE=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    get_pages_revisions.py\n",
    "\n",
    "    MediaWiki API Demos\n",
    "    Demo of `Revisions` module: Get revision data with content for pages\n",
    "    with titles [[API]] and [[Main Page]]\n",
    "\n",
    "    MIT License\n",
    "\"\"\"\n",
    "relation=['P6379', 'P749', 'P366', 'P739', 'P121', 'P4950', \n",
    "'P131', 'P495', 'P625', 'P355', 'P463', 'P5009', 'P17', 'P414',\n",
    " 'P1830', 'P452', 'P306', 'P176', 'P169', 'P199', 'P1344', 'P156',\n",
    "  'P2770', 'P3320', 'P113', 'P1454', 'P159', 'P740', 'P114', 'P31',\n",
    "   'P1056', 'P1889', 'P166', 'P400', 'P361', 'P155', 'P127', 'P112', 'P793']\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "VERSION\t\t\t\t\t= \"0.1\"\n",
    "USER_AGENT\t\t\t= \"Mozilla/5.0 (compatible; Awena/\"+VERSION+\"; +https://github.com/sedthh/awena-wikidata-crawler/)\"\n",
    "# S = requests.Session()\n",
    "triple_list=list()\n",
    "year=2015\n",
    "day_month=['2018-01','2018-12','2019-06','2019-12','2020-06','2020-12','2021-06','2021-12','2022-06']\n",
    "for period in day_month:\n",
    "    for revid in DATE[period].values():\n",
    "        URL = \"https://www.wikidata.org/w/api.php\"\n",
    "        headers\t= {\n",
    "                    \"User-Agent\"\t: USER_AGENT\n",
    "                }\n",
    "        PARAMS = {\n",
    "                \"action\":'parse',\n",
    "                \"format\":\"json\",\n",
    "                \"oldid\":revid,\n",
    "                \"prop\":\"wikitext\"\n",
    "            }\n",
    "        R=requests.get(\"https://www.wikidata.org/w/api.php\",headers=headers,params=PARAMS)\n",
    "        # R = S.get(url=URL, params=PARAMS)\n",
    "\n",
    "        INFO = R.json()\n",
    "        DATA=json.loads(INFO['parse']['wikitext']['*'])\n",
    "        for r in DATA['claims'].keys():\n",
    "            if r in relation:\n",
    "                for i in range(len(DATA['claims'][r])):\n",
    "                    sample=dict()\n",
    "                    sample['wiki_code']=DATA['id']\n",
    "                    sample['relation']=r\n",
    "                    try:\n",
    "                        sample['node']=DATA['claims'][r][i]['mainsnak']['datavalue']['value']['id']\n",
    "                        triple_list.append(sample)\n",
    "                    except:\n",
    "                        continue\n",
    "    df=pd.DataFrame(triple_list,columns=['wiki_code','relation','node'])\n",
    "    df.to_csv(day+'/triple_'+period+'.csv')\n",
    "    # file_name='./data/triple_'+period+'.csv'\n",
    "    # triple=open(file_name,'w',encoding='UTF-8')\n",
    "    # fieldnames=['wiki_code','relation','node']\n",
    "    # writer=csv.DictWriter(triple,fieldnames=fieldnames)\n",
    "    # writer.writeheader()\n",
    "    # writer.writerows(triple_list)\n",
    "    year+=1\n",
    "                \n",
    "    # DATA['parse']['wikitext']['*']\n",
    "# DATA=DATA['parse']['wikitext']['*']\n",
    "# data_list=DATA.split()\n",
    "\n",
    "# PAGES = DATA[\"query\"][\"pages\"]\n",
    "# PAGES\n",
    "# for page in PAGES:\n",
    "#     print(page[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "with open(\"./\"+day+\"/wiki_ID.json\") as f:\n",
    "    ticker_entityID=json.load(f) \n",
    "data=[]\n",
    "for key in ticker_entityID.keys():\n",
    "    sample={}\n",
    "    sample['ticker']=key\n",
    "    sample['wiki_code']=ticker_entityID[key]\n",
    "    data.append(sample)\n",
    "df=pd.DataFrame(data,columns=['ticker','wiki_code'])\n",
    "df.to_csv('./'+day+'/company_480.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce2c101e633a132a2be35be8e919a9e0dab56f707b035eb5d9ffe17010837cfa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
